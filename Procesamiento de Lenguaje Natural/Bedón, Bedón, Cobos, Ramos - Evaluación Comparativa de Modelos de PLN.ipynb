{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc08a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy spacy-conll\n",
    "# %pip install transformers torch seqeval\n",
    "# %%python -m spacy download en_core_web_sm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682037a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b773feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                parts = token.split() # Divide por cualquier espacio en blanco\n",
    "                if len(parts) >= 4:\n",
    "                    # parts[0] es la palabra, parts[3] es la etiqueta NER completa\n",
    "                    word = parts[0]\n",
    "                    full_label = parts[3] \n",
    "                    \n",
    "                    # Quitamos los prefijos B- o I- para que no ensucien la métrica\n",
    "                    clean_label = full_label.replace(\"B-\", \"\").replace(\"I-\", \"\")\n",
    "                    \n",
    "                    token_data.append([word, clean_label])\n",
    "            if token_data:\n",
    "                data.append(token_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_format(data):\n",
    "    \"\"\"Convierte el formato de columnas CoNLL a (texto, {'entities': [(s, e, label)]})\"\"\"\n",
    "    formatted_data = []\n",
    "    for sentence in data:\n",
    "        full_text = \"\"\n",
    "        entities = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        for token_parts in sentence:\n",
    "            # En eng.testa: [0]=Palabra, [-1]=Etiqueta NER\n",
    "            word = token_parts[0]\n",
    "            ner_tag = token_parts[-1]\n",
    "            \n",
    "            start = current_pos\n",
    "            end = start + len(word)\n",
    "            full_text += word + \" \"\n",
    "            current_pos = end + 1 # +1 por el espacio\n",
    "            \n",
    "            if ner_tag != 'O':\n",
    "                # Limpiamos el prefijo B- o I- para dejar solo la etiqueta (LOC, ORG, etc.)\n",
    "                label = ner_tag.split('-')[-1]\n",
    "                entities.append((start, end, label))\n",
    "        \n",
    "        formatted_data.append((full_text.strip(), {\"entities\": entities}))\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1a5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "raw_data = read_conll_file(r\"C:\\Users\\arbed\\Downloads\\archive\\conll2003\\eng.testa\")\n",
    "processed_data = get_spacy_format(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f99e4f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 3466 oraciones...\n"
     ]
    }
   ],
   "source": [
    "scorer = Scorer()\n",
    "examples = []\n",
    "\n",
    "print(f\"Procesando {len(processed_data)} oraciones...\")\n",
    "\n",
    "for text, annotations in processed_data:\n",
    "    # El modelo predice sobre el texto\n",
    "    doc_pred = nlp(text)\n",
    "    # Creamos el objeto Example para comparar predicción vs realidad\n",
    "    try:\n",
    "        example = Example.from_dict(doc_pred, annotations)\n",
    "        examples.append(example)\n",
    "    except Exception as e:\n",
    "        # Ignorar si hay desajustes menores en tokens\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a623d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "         RESULTADOS NER (en_core_web_sm)          \n",
      "==================================================\n",
      "\n",
      "Entidad         | Precisión  | Recall     | F1-Score  \n",
      "--------------------------------------------------\n",
      "ORG             |    28.56% |    13.53% |    18.36%\n",
      "GPE             |     0.00% |     0.00% |     0.00%\n",
      "DATE            |     0.00% |     0.00% |     0.00%\n",
      "LOC             |    61.97% |     2.10% |     4.06%\n",
      "NORP            |     0.00% |     0.00% |     0.00%\n",
      "PERSON          |     0.00% |     0.00% |     0.00%\n",
      "CARDINAL        |     0.00% |     0.00% |     0.00%\n",
      "FAC             |     0.00% |     0.00% |     0.00%\n",
      "MISC            |     0.00% |     0.00% |     0.00%\n",
      "PER             |     0.00% |     0.00% |     0.00%\n",
      "TIME            |     0.00% |     0.00% |     0.00%\n",
      "ORDINAL         |     0.00% |     0.00% |     0.00%\n",
      "LANGUAGE        |     0.00% |     0.00% |     0.00%\n",
      "LAW             |     0.00% |     0.00% |     0.00%\n",
      "MONEY           |     0.00% |     0.00% |     0.00%\n",
      "EVENT           |     0.00% |     0.00% |     0.00%\n",
      "PERCENT         |     0.00% |     0.00% |     0.00%\n",
      "PRODUCT         |     0.00% |     0.00% |     0.00%\n",
      "WORK_OF_ART     |     0.00% |     0.00% |     0.00%\n",
      "QUANTITY        |     0.00% |     0.00% |     0.00%\n",
      "--------------------------------------------------\n",
      "GLOBAL F1-SCORE:     3.73%\n",
      "GLOBAL PRECISION:    3.66%\n",
      "GLOBAL RECALL:       3.80% (Exactitud)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "scores = scorer.score(examples)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'RESULTADOS NER (en_core_web_sm)':^50}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Métricas por tipo de entidad\n",
    "print(f\"\\n{'Entidad':<15} | {'Precisión':<10} | {'Recall':<10} | {'F1-Score':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for label, m in scores[\"ents_per_type\"].items():\n",
    "    print(f\"{label:<15} | {m['p']:>9.2%} | {m['r']:>9.2%} | {m['f']:>9.2%}\")\n",
    "\n",
    "# Métricas globales\n",
    "print(\"-\" * 50)\n",
    "print(f\"GLOBAL F1-SCORE:  {scores['ents_f']:>8.2%}\")\n",
    "print(f\"GLOBAL PRECISION: {scores['ents_p']:>8.2%}\")\n",
    "print(f\"GLOBAL RECALL:    {scores['ents_r']:>8.2%} (Exactitud)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e71c2894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\" # Modelo optimizado para CoNLL-2003\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "nlp_bert = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            words, labels = [], []\n",
    "            for token in tokens:\n",
    "                parts = token.split()\n",
    "                if len(parts) > 0:\n",
    "                    words.append(parts[0])\n",
    "                    labels.append(parts[-1])\n",
    "            if words:\n",
    "                data.append({\"words\": words, \"labels\": labels})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840ec036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando BERT sobre 3466 oraciones...\n"
     ]
    }
   ],
   "source": [
    "raw_data = read_conll_file(r\"C:\\Users\\arbed\\Downloads\\archive\\conll2003\\eng.testa\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(f\"Evaluando BERT sobre {len(raw_data)} oraciones...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f6cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in raw_data[:200]: # Limitamos a 200 para rapidez, puedes quitar el slice\n",
    "    sentence = \" \".join(entry['words'])\n",
    "    gold_labels = entry['labels']\n",
    "    \n",
    "    # Predicción de BERT\n",
    "    outputs = nlp_bert(sentence)\n",
    "    \n",
    "    # BERT devuelve entidades encontradas, necesitamos mapearlas \n",
    "    # de nuevo a la estructura original de palabras para comparar\n",
    "    predicted_labels = [\"O\"] * len(entry['words'])\n",
    "    \n",
    "    for ent in outputs:\n",
    "        # Buscamos a qué palabra(s) corresponde la entidad detectada\n",
    "        # (Aproximación simple por coincidencia de texto)\n",
    "        for i, word in enumerate(entry['words']):\n",
    "            if ent['word'] in word or word in ent['word']:\n",
    "                predicted_labels[i] = ent['entity_group']\n",
    "    \n",
    "    # Estandarizar etiquetas (BERT usa 'PER', el dataset 'B-PER')\n",
    "    y_true.append(gold_labels)\n",
    "    # Limpiamos prefijos B- e I- de la verdad para comparar con la salida simple de BERT\n",
    "    # y_true[-1] = [label.split('-')[-1] for label in y_true[-1]]\n",
    "    y_pred.append(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a981d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = sorted(list(set([lbl for sublist in y_true for lbl in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1782bcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "           REPORT DE MÉTRICAS BERT NER            \n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ER       0.00      0.00      0.00         0\n",
      "         ISC       0.00      0.00      0.00         0\n",
      "         LOC       0.00      0.00      0.00       110\n",
      "        MISC       0.00      0.00      0.00        27\n",
      "          OC       0.00      0.00      0.00         0\n",
      "         ORG       0.00      0.00      0.00       119\n",
      "         PER       0.00      0.00      0.00       108\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       364\n",
      "   macro avg       0.00      0.00      0.00       364\n",
      "weighted avg       0.00      0.00      0.00       364\n",
      "\n",
      "--------------------------------------------------\n",
      "Global F1-Score:  0.00%\n",
      "Global Precision: 0.00%\n",
      "Global Recall:    0.00%\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arbed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arbed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: MISC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arbed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arbed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arbed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\arbed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'REPORT DE MÉTRICAS BERT NER':^50}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Global F1-Score:  {f1_score(y_true, y_pred):.2%}\")\n",
    "print(f\"Global Precision: {precision_score(y_true, y_pred):.2%}\")\n",
    "print(f\"Global Recall:    {recall_score(y_true, y_pred):.2%}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
